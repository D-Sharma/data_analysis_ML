---
title: "Machine Learning Project: Click Through Rate"
author: "Ankesh Tyagi           : tyagi.ankesh@gmail.com 
         Arafat Gabareen Mokhtar: gamo.arafat@gmail.com  
         Dimple Sharma          : dimple.narang@gmail.com
         Rodrigo Zuniga         : rodrigozuniga@gmail.com
         Vaibhav chaudhary      : chvaibhav79@hotmail.com"
date: "November 10, 2014"
output: html_document
framework: revealjs

---

## Click Through Rate: Data from Criteo Labs

Dataset consists of user visits on a website. Data has 40 predictor variables
and 1 response variable. This a classifciation problem


Presented By: Ankesh, Arafat, Dimple, Rodrigo and Vaibhav

---

## Approach Used

We have implemented 4 different models
- Logistic Regression
- Decision Trees
- Support Vector Machines
- Random Forest

---

```{r, echo=TRUE}
rm(list=ls())
libs = c('sqldf', 'e1071', 'bestglm', 'gbm', 'rpart', 'randomForest', 'ridge', 'devtools', 'slidify')
run_libs = lapply(libs, require, character.only=T)
par(mar = c(5,5,2.,2.) + 0.1)
par(pch=19, lty=1, col.axis='blue', col.lab='red', lwd=2, cex.axis=1.4, font.lab=3, cex.lab=1.6)
options(scipen = 999)
options(ndigits=3)
```

---

```{r, echo=TRUE}
file='/Users/mokhtar/Arafat/ML_UCSC/Project/Data/train_wo_NAs.txt'
data = read.table(file=file,header=T, stringsAsFactors=F, sep='\t')

data=data[,1:40]
nm=c('V1')
for(i in 2:ncol(data)){
    nm= c(nm,paste0('V',i))
}
colnames(data) = nm
data$V1 = as.factor(data$V1)
ddd = na.omit(data)

vars = data.frame(var=character(0), lines=integer(0))
for(i in 15:ncol(data)){
    query = paste0('select V',i,' , count(*) from ddd group by 1 order by 2 desc')
    check= sqldf(query)
    var  = paste0('V',i)
    levels= nrow(check)
    df = data.frame(var=var, lines=levels)
    vars = rbind(vars, df)
}
vars = vars[with(vars, order(-lines)),]

important_vars = vars[ vars$lines<=10,]
colnames(important_vars) = c("Attribute","Unique Levels")
print(important_vars)

lr_test = glm(V1~V20+V23+V31+V34+V36, data = data, family=binomial)
```

---
```{r, echo=TRUE}
n1 = as.integer(nrow(ddd)*2/3)
n2 = as.integer(nrow(ddd))
train = ddd[1:n1,]
test  = ddd[(n1+1):n2,]
```

---
```{r, echo=TRUE}
#============ Let's remove all the observations from the test sample where they don't have base in the training sample
for(vvv in c('V20','V23','V31','V34','V36')){
  print(vvv)
  ind = which(colnames(train) %in% vvv)
  query = paste0('select distinct ',vvv, ' from test')
  unique_test  = sqldf(query)
  query = paste0('select distinct ',vvv, ' from train')
  unique_train = sqldf(query) 
  # Find all the cases in the test but not in train
  for( case in unique_test[,1]){
    if(case %in% unique_train[,1]){
      x='pass'
      }else{
        print(case)
        ind   = which(colnames(train) %in% vvv)
        test = test[ test[,ind]!=case, ]
    }
  }
  
}
```

---
```{r, echo=TRUE, results='hide',message=TRUE}
#============================================   Let's Run the logistic Regression Model
kk = scale(x = train[,2:14], center = T, scale = T)
lr         = glm(V1~V2+V3+V4+V5+V6+V7+V8+V9+V10+V11+V12+V14+V23+V20+V31+V34+V36, data=train, family = "binomial")
best = step(lr, direction = 'both', steps = 1000)
pred = predict(best, newdata = test[,-1])
hh   = ifelse(pred>0.5, 1, 0)
err = 1- sum(hh==test$V1)/length(hh)
```

```{r, echo=TRUE}
print(paste('Error from Logistic Regression = ',format(round(err,3), digits=3)))
```

---
```{r, echo=TRUE}
lr_result = summary(best)
lr_result = as.data.frame(as.matrix(lr_result$coefficients))
lr_result[,1] =format(lr_result[,1],scientific = T, trim=T, digits=2)
lr_result[,2] =format(lr_result[,2],scientific = T, trim=T, digits=2)
lr_result[,3] =format(lr_result[,3],scientific = T, trim=T, digits=2)
lr_result[,4] =format(lr_result[,4],scientific = T, trim=T, digits=2)
colnames(lr_result) = c('Estimate','Standard_Error','Z_Value','P_Value')
lr_result$Significant = ifelse(as.numeric(lr_result$P_Value)<0.05, 
                               'Yes','No') 
```

---
```{r, echo=TRUE}
print(lr_result)
```

---

```{r, echo=TRUE}
Checks = data.frame(Test_Predict = pred)
Checks$flag = ifelse(Checks$Test_Predict>0.5, 1, 0)
Checks = cbind(Checks, Test_data= test$V1)
Checks$TP = ifelse(Checks$flag==1&Checks$Test_data==1, 1, 0) # True Positive
Checks$FN = ifelse(Checks$flag==0&Checks$Test_data==1, 1, 0) # False Negative
Checks$FP = ifelse(Checks$flag==1&Checks$Test_data==0, 1, 0) # False Positive
Checks$TN = ifelse(Checks$flag==0&Checks$Test_data==0, 1, 0) # True Negative
```

---
```{r, echo=TRUE}
plot(y=cumsum(Checks$TP)/sum(Checks$TP), x=cumsum(Checks$FP)/sum(Checks$FP),
     xlab='False Positive', ylab = 'True Positive',
     main='True Positive versus False Positive', type='l',
     col='red', lwd=4)
abline(0,1, col='blue', lty=4, lwd=5)
```

---
## Decision Tree
```{r, echo=TRUE}
#file='~/Box Sync/UCSC-Machine Learning/Project/sample20K.txt'
data1 <- read.table(file=file, header = F, stringsAsFactors = T, sep = '\t',
                    na.strings=c("", "NA"), skip = 1)
data<-data1[,2:42]
nm=c('V1')
for(i in 2:41){
  nm= c(nm,paste0('V',i))
}
colnames(data) = nm
#head(data)
data$V1 <- as.factor(data$V1)
d <- na.omit(data)
vari <- c('V1','V2','V3','V4','V5','V6','V7','V8','V9','V10',
          'V11','V12','V13','V14', 'V23', 'V34', 'V36',
          'V20', 'V31')
#Use only selected columns from the entire dataset
dat <- d[,vari]
dim(dat)
str(dat)
train<-dat[1:2000,]
test<-dat[2001:2453,]
nxval <- 10  # this is 10 fold cross validation
ndepth <- 20 # this creates trees from depth 1 to depth 10
trainOutput <- matrix(0.0,nrow = ndepth, ncol = 2)#Set up the matrix to hold the scores
testOutput <- matrix(0.0,nrow =ndepth, ncol = 2)
I <- seq(from = 1, to = nrow(train))
```

---

```{r, echo=TRUE}
for(idepth in 1:ndepth){
  trainErr <- 0.0 
  testErr <- 0.0   
  for(ixval in seq(from =  1, to = nxval)){
    Iout <- which(I%%nxval == ixval%%nxval)
    trainIn <- train[-Iout,]
    trainOut <- train[Iout,]
    yin <- as.ordered(trainIn[,1])
    yout <- as.ordered(trainOut[,1])
    xin <- trainIn[,-1]
    xout <- trainOut[,-1]
    #class(xin)
    fit <- rpart(yin~.,xin,control=rpart.control(maxdepth=idepth, minsplit=2))
    #Calculate the training error
    trainErr <- trainErr + (1-sum(yin==ordered(predict(fit,xin,type="class"),levels=levels(yin)))/length(yin))
    #Calculate the test error
    testErr <- testErr + (1-sum(yout==ordered(predict(fit,xout,type="class"),levels=levels(yout)))/length(yout))
  }
  trainOutput[idepth,1] <- idepth
  trainOutput[idepth,2] <- trainErr/nxval
  testOutput[idepth,1] <- idepth
  testOutput[idepth,2] <- testErr/nxval
}
```
Contd..

---
```{r,message = FALSE}
## Contd..
maxval = max(testOutput[,2])
plot(trainOutput, ylim=c(0,maxval),
     main="Model Complexity",
     xlab="Model Complexity = Tree Depth",
     ylab="Prediction Error"
)
legend("right", c("test", "train"), col = c(2,1), pch=1,cex=0.6)
points(testOutput, col = 2)
```

---

```{r, echo=TRUE}
index <- which.min(testOutput[,2]) 
testOutput[index,2]  # 0.281
index  # 3
print(paste('Optimal depth of a tree = ',index))
print(paste('Error from trees = ',round(testOutput[index,2],3)))

fit= rpart(V1~.,train,control=rpart.control(maxdepth=3, minsplit=2))

summary(fit)
pred=predict(fit,newdata=test,type="class")
err=1-sum(test[,1]==predict(fit,newdata=test,type="class"))/length(test[,1])
print(paste('Error test set = ',err)) #0.28256
```

---
## Decision Tree -2
```{r, , echo=TRUE}
#file='~/Box Sync/UCSC-Machine Learning/Project/sample20K.txt'
ddd <- read.table(file=file,
                   header = T, stringsAsFactors = T, sep = '\t',
                   na.strings=c("", "NA"))
ddd = cbind(x=rep(1,nrow(ddd)), ddd)
yhat3<-c()
yhat3b<-c()
set.seed(10)
numvar=15
numtree=10
```

---
```{r, , echo=TRUE}
#Get predictions from [numtree] different trees with randomly selected variables
for (i in 1:numtree){
    t1<-rpart(factor(V1)~.,data=ddd[1:15000,c(2,sample(3:40,numvar))],control=rpart.control(maxdepth=10),method='class',parms=list(prior=c(.5,.5)))
    yhat<-predict(t1,ddd[1:15000,])
    yhat2<-(yhat[,2]>yhat[,1])*1
    yhat3<-cbind(yhat3,yhat2)   
    yhatb<-predict(t1,ddd[15001:20000,])
    yhat2b<-(yhatb[,2]>yhatb[,1])*1
    yhat3b<-cbind(yhat3b,yhat2b)
}
```

---
```{r, , echo=TRUE}
#rename variables
colnames(yhat3)<-paste("V",1:numtree,sep="")
colnames(yhat3b)<-paste("V",1:numtree,sep="")
#get dependent variable from ddd
y<-ddd[1:15000,2]
yb<-ddd[15001:20000,2]
#add y to the predicted values 
yhat3<-as.data.frame(cbind(yhat3,y))
yhat3b<-as.data.frame(cbind(yhat3b,yb))
```

---
```{r, , echo=TRUE}
#create linear ridge model using the predictions from each tree from the train dataset
lR<-linearRidge(y~.,data=yhat3,scaling="scale",lambda=3)
#predict dependent variable
yhat4<-predict(lR,newdata=yhat3b)
#transform prediction into 0 and 1s
yhat5<-(yhat4>0.5)*1
#get error message
1-sum(yhat5==yhat3b[,numtree+1])/length(yhat5)
#confusion table: false positives & false negatives
table(yhat5,yhat3b[,numtree+1])

```

---
## Ensemble Methods - RandomForest
```{r, echo=TRUE}
#file='~/Box Sync/UCSC-Machine Learning/Project/sample20K.txt'
data <- read.table(file=file,
                   header = T, stringsAsFactors = T, sep = '\t',
                   na.strings=c("", "NA"))
ddd = na.omit(data)
indices = c('V1','V2','V3','V4','V5','V6','V7','V8','V9','V10','V11','V12','V14','V23'
            ,'V20','V31','V34','V36')
ddd = ddd[,indices]
ddd$V1 = as.factor(ddd$V1)
set.seed(101)
ind <- sample(2, nrow(ddd), replace = TRUE, prob=c(0.8, 0.2))
```

---
### Implement Random Forest
```{r, echo=TRUE}
#rF = randomForest(x= train[,-1],y = train[,1], xtest = test[,-1], ytest = test[,1])
rF = randomForest(V1~., data = ddd[ind == 1,])
class(rF)
pred = predict(rF, ddd[ind == 2,])
head(pred)
err = 1 - sum((pred==ddd[ind == 2, 1]))/length(pred)
print(paste0('Error from RandomForset with 500 trees: ',round(err,3)))
```

---
### Try multiple trees in random forest
```{r, echo=TRUE}
error = data.frame(0,0)
names(error) = c('Number of Trees','Error')
for (tree in 500:510){
  rF = randomForest(V1~., data = ddd[ind == 1,], ntree = tree)
  #class(rF)
  pred = predict(rF, ddd[ind == 2,])
  head(pred)
  error[tree,1] = tree
  error[tree,2] = 1 - sum((pred==ddd[ind == 2, 1]))/length(pred)
}
```

---
```{r, echo=TRUE}
plot(error[500:510,1], error[500:510,2], ylab="Error", xlab = "Number of Trees", pch = 4,col = 'red', main = "Number of trees vs Error")
```

Increasing number of trees doesnt necessarily improve classification accuracy

